# Notebook 10 - Road to the top, part 3

1. TOC
{:toc}

## What did I do?
In this Notebook Jerermy continues to fine tune the model performance for the kaggle competition. Initially this is through altering the fine-tune argument and implementing 
gradient accumulation (model weights accumulate). This model also runs through GPU as opposed to being CPU drivin. This means that the memory allocation needs to be checked.
A number of different architectures where trained, and ensembelling was implemented. This produced very good performance results for the kaggle competition.

## What did I learn?
- While image processing seems complecated, good model performance can be achieved through small amounts of code.
- Their are numerous standardised ways to improve model performance.

> Workbook10 is completed!
